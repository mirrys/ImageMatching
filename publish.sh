#!/usr/bin/env bash
# Run the ImageRecommendation algo via algorunner.py, and generate production datasets
# for all languages defined in `wikis`.
#  
# The intermediate algo output and the production datasets will be stored in HDFS
# and exposed as Hive external tables:
#
# - <username>.imagerec: raw datasets (algo output). Maps to hdfs:///users/<username>/imagerec
# - <username>.imagerec_prod: production datasets. Maps to hdfs:///users/<username>/imagerec_prod
# 
# Where <username> is the user currently running the publish.sh script.
#
# Production datasets will be exported locally, in tsv format, under runs/<run_id>/imagerec_prod_${snapshot}.
#
# Each time publish.sh is invoked, it records the following data under runs/<run_id>:
#
# - metrics: a set of timing metrics generated by this script
# - Output: raw model output in tsv format
# - imagerec_prod_${snapshot}: production datasets in tsv format
# - regular.spark.properties: spark properties file for the transform.py job
#
# Each run has an associated, unique, <run_id>. This uuid is propagated to the etl transforms,
# and will populate the `dataset_id` in production datasets. This allows reconciliation of
# a given dataset to the process that generated it.
#
# Usage: ./publish.sh <snapshot>
# Example: ./publish.sh 2021-01-25

snapshot=$1

run_id=$(cat /proc/sys/kernel/random/uuid)


# Target wikis to train ImageMatching on
wikis="aawiki abwiki acewiki adywiki afwiki akwiki alswiki amwiki angwiki anwiki arcwiki arwiki arywiki arzwiki astwiki aswiki atjwiki avkwiki avwiki awawiki aywiki azbwiki azwiki banwiki barwiki bat_smgwiki bawiki bclwiki bewiki bgwiki bhwiki biwiki bjnwiki bmwiki bnwiki bowiki bpywiki brwiki bswiki bugwiki bxrwiki cawiki cdowiki cebwiki cewiki chowiki chrwiki chwiki chywiki ckbwiki cowiki crhwiki crwiki csbwiki cswiki cuwiki cvwiki cywiki dawiki dewiki dinwiki diqwiki donatewiki dsbwiki dtywiki dvwiki dzwiki eewiki elwiki emlwiki enwiki eowiki eswiki etwiki euwiki extwiki fawiki ffwiki fiu_vrowiki fiwiki fjwiki fowiki frpwiki frrwiki frwiki furwiki fywiki gagwiki ganwiki gawiki gcrwiki gdwiki glkwiki glwiki gnwiki gomwiki gorwiki gotwiki guwiki gvwiki hakwiki hawiki hawwiki hewiki hifwiki hiwiki howiki hrwiki hsbwiki htwiki huwiki hywiki hywwiki hzwiki iawiki idwiki iewiki igwiki iiwiki ikwiki ilowiki incubatorwiki inhwiki iowiki iswiki itwiki iuwiki jamwiki jawiki jbowiki jvwiki kaawiki kabwiki kawiki kbdwiki kbpwiki kgwiki kiwiki kjwiki kkwiki klwiki kmwiki knwiki koiwiki kowiki krcwiki krwiki kshwiki kswiki kuwiki kvwiki kwwiki kywiki ladwiki lawiki lbewiki lbwiki lezwiki lfnwiki lgwiki lijwiki liwiki lldwiki lmowiki lnwiki lowiki lrcwiki ltgwiki ltwiki lvwiki maiwiki map_bmswiki mdfwiki mediawikiwiki metawiki mgwiki mhrwiki mhwiki minwiki miwiki mkwiki mlwiki mnwiki mnwwiki mrjwiki mrwiki mswiki mtwiki muswiki mwlwiki myvwiki mywiki mznwiki nahwiki napwiki nawiki nds_nlwiki ndswiki newiki newwiki ngwiki nlwiki nnwiki novwiki nowiki nqowiki nrmwiki nsowiki nvwiki nywiki ocwiki olowiki omwiki orwiki oswiki pagwiki pamwiki papwiki pawiki pcdwiki pdcwiki pflwiki pihwiki piwiki plwiki pmswiki pnbwiki pntwiki pswiki ptwiki quwiki rmwiki rmywiki rnwiki roa_rupwiki roa_tarawiki rowiki ruewiki ruwiki rwwiki sahwiki satwiki sawiki scnwiki scowiki scwiki sdwiki sewiki sgwiki shnwiki shwiki simplewiki siwiki skwiki slwiki smwiki snwiki sourceswiki sowiki specieswiki sqwiki srnwiki srwiki sswiki stqwiki stwiki suwiki svwiki swwiki szlwiki szywiki tawiki tcywiki tenwiki test2wiki testwiki tetwiki tewiki tgwiki thwiki tiwiki tkwiki tlwiki tnwiki towiki tpiwiki trwiki tswiki ttwiki tumwiki twwiki tyvwiki tywiki udmwiki ugwiki ukwiki urwiki uzwiki vecwiki vepwiki vewiki viwiki vlswiki votewiki vowiki warwiki wawiki wowiki wuuwiki xalwiki xhwiki xmfwiki yiwiki yowiki zawiki zeawiki zh_classicalwiki zh_min_nanwiki zh_yuewiki zhwiki zuwiki"

# wikis to export for PoC
poc_wikis="enwiki arwiki kowiki cswiki viwiki frwiki fawiki ptwiki ruwiki trwiki plwiki hewiki svwiki ukwiki huwiki hywiki srwiki euwiki arzwiki cebwiki dewiki bnwiki"

# YYYY-MM
monthly_snapshot=$(echo ${snapshot} | awk -F'-' '{print $1"-"$2}')
username=$(whoami)

# Path were raw dataset (Jupyter algo output) will be stored
algo_outputdir=runs/${run_id}/Output

# Path on the local filesystem where production datasets will be stored.
outputdir=runs/${run_id}/imagerec_prod_${snapshot}

make venv
source venv/bin/activate

mkdir -p $(pwd)/runs/${run_id}/
metrics_dir=$(pwd)/runs/${run_id}/metrics
mkdir -p $metrics_dir
echo "Starting training run ${run_id} for snapshot=$snapshot. Model artifacts will be collected under
$(pwd)/runs/${run_id}"

# TODO(gmodena, 2021-02-02): 
# Passing one wiki at a time to get a feeling for runtime deltas (to some degree, we could get this info from parsing hdfs snapshots).
# We could pass the whole list at algorunner.py at once,
# and have the pipeline run on a single (long running) spark job. Instead, here we
# are spinning up one spark cluster per wiki. This needs checking with AE, in order
# to better understand which workload better fits our Hadoop cluster.
for wiki in ${wikis}; do
	# 1. Run the algo and generate data locally
	echo "Generating recommendations for ${wiki}"
	STARTTIME=${SECONDS}
	python algorunner.py ${snapshot} ${wiki} ${algo_outputdir}
	ENDTIME=${SECONDS}
	metric_name=metrics.algorunner.${wiki}.${snapshot}.seconds
        timestamp=$(date +%s)
	echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}
	

	# 2. Upload to HDFS
        echo "Publishing raw data to HDFS for ${wiki}"
	STARTTIME=${SECONDS}
	hadoop fs -rm -r imagerec/data/wiki_db=${wiki}/snapshot=${monthly_snapshot}/
	hadoop fs -mkdir -p imagerec/data/wiki_db=${wiki}/snapshot=${monthly_snapshot}/

	hadoop fs -copyFromLocal ${algo_outputdir}/${wiki}_${snapshot}_wd_image_candidates.tsv imagerec/data/wiki_db=${wiki}/snapshot=${monthly_snapshot}/
	ENDTIME=${SECONDS}
	metric_name=metrics.hdfs.copyrawdata.${wiki}.${snapshot}.seconds
        timestamp=$(date +%s)
	echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}

	# 3. Update hive external table metadata
	echo "Updating Hive medatada for ${wiki}" 
	STARTTIME=${SECONDS}
	hive -hiveconf username=${username} -f ddl/external_imagerec.hql
done
	ENDTIME=${SECONDS}
	timestamp=$(date +%s)
	metric_name=metrics.hive.imagerec.${wiki}.${snapshot}.seconds
	echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}
# 4. Submit the Spark production data ETL
echo "Generating production data"

## Generate spark config
spark_config=runs/$run_id/regular.spark.properties
cat conf/spark.properties.template /usr/lib/spark2/conf/spark-defaults.conf > ${spark_config}

STARTTIME=${SECONDS}
spark2-submit --properties-file ${spark_config} etl/transform.py \
	--snapshot ${monthly_snapshot} \
	--source imagerec/data/ \
	--destination imagerec_prod/data/ \
	--dataset-id ${run_id}
ENDTIME=${SECONDS}
metric_name=metrics.etl.transfrom.${snapshot}.second
timestamp=$(date +%s)
echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}


# 5. Update hive external table metadata (production)
STARTTIME=${SECONDS}
hive -hiveconf username=${username} -f ddl/external_imagerec_prod.hql
ENDTIME=${SECONDS}
metric_name=hive.imagerec_prod.${snapshot}
timestamp=$(date +%s)
echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}

# 6. Export production datasets
STARTIME=${SECONDS}
mkdir ${outputdir}
for wiki in ${poc_wikis}; do
	hive -hiveconf username=${username} -hiveconf wiki=${wiki} -hiveconf snapshot=${monthly_snapshot} -f ddl/export_prod_data.hql > ${outputdir}/prod-${wiki}-${snapshot}-wd_image_candidates.tsv
done
ENDTIME=${SECONDS}
echo "Datasets are available at $outputdir/"
metric_name=metrics.etl.export_prod_data.${snapshot}.seconds
timestamp=$(date +%s)
echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}

echo "Export summary"
cut -f 3,4 ${outputdir}/*.tsv | sort -k 1,2 | uniq -c 
