#!/usr/bin/env bash
# Run the ImageRecommendation algo via algorunner.py, and generate production datasets
# for all languages defined in `wikis`.
#  
# The intermediate algo output and the production datasets will be stored in HDFS
# and exposed as Hive external tables:
#
# - <username>.imagerec: raw datasets (algo output). Maps to hdfs:///users/<username>/imagerec
# - <username>.imagerec_prod: production datasets. Maps to hdfs:///users/<username>/imagerec_prod
# 
# Where <username> is the user currently running the publish.sh script.
#
# Production datasets will be exported locally, in tsv format, under runs/<run_id>/imagerec_prod_${snapshot}.
#
# Each time publish.sh is invoked, it records the following data under runs/<run_id>:
#
# - metrics: a set of timing metrics generated by this script
# - Output: raw model output in tsv format
# - imagerec_prod_${snapshot}: production datasets in tsv format
# - regular.spark.properties: spark properties file for the transform.py job
#
# Each run has an associated, unique, <run_id>. This uuid is propagated to the etl transforms,
# and will populate the `dataset_id` in production datasets. This allows reconciliation of
# a given dataset to the process that generated it.
#
# Usage: ./publish.sh <snapshot>
# Example: ./publish.sh 2021-01-25

snapshot=$1

run_id=$(cat /proc/sys/kernel/random/uuid)

# YYYY-MM
monthly_snapshot=$(echo ${snapshot} | awk -F'-' '{print $1"-"$2}')
username=$(whoami)

# Populate the script with `wiki` and `poc_wiki` variables.
# Those vars hold a list of wikis to train on, and to export to PoC clients
# respectively
source conf/wiki.conf

# Path were raw dataset (Jupyter algo output) will be stored
algo_outputdir=runs/${run_id}/Output

# Path on the local filesystem where production datasets will be stored.
outputdir=runs/${run_id}/imagerec_prod_${snapshot}

# Temporary directory where data collected from Hive during the 
# production datasets export is stored.
tsv_tmpdir=runs/${run_id}/tmp

make venv
source venv/bin/activate
wikis=${all_wikis}
poc_wikis=${target_wikis}

mkdir -p $(pwd)/runs/${run_id}/
metrics_dir=$(pwd)/runs/${run_id}/metrics
mkdir -p $metrics_dir
echo "Starting training run ${run_id} for snapshot=$snapshot. Model artifacts will be collected under
$(pwd)/runs/${run_id}"

## Generate spark config
spark_config=runs/$run_id/regular.spark.properties
cat conf/spark.properties.template /usr/lib/spark2/conf/spark-defaults.conf > ${spark_config}

# Generate placeholder_images parquet file
echo "Generating placeholder_image parquet file"
STARTTIME=${SECONDS}
spark2-submit --properties-file ${spark_config} placeholder_images.py ${monthly_snapshot}
ENDTIME=${SECONDS}
metric_name=metrics.placeholder_images.${monthly_snapshot}.seconds
timestamp=$(date +%s)
echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}

# TODO(gmodena, 2021-02-02): 
# Passing one wiki at a time to get a feeling for runtime deltas (to some degree, we could get this info from parsing hdfs snapshots).
# We could pass the whole list at algorunner.py at once,
# and have the pipeline run on a single (long running) spark job. Instead, here we
# are spinning up one spark cluster per wiki. This needs checking with AE, in order
# to better understand which workload better fits our Hadoop cluster.
for wiki in ${wikis}; do
	# 1. Run the algo and generate data locally
	echo "Generating recommendations for ${wiki}"
	STARTTIME=${SECONDS}
	python algorunner.py ${snapshot} ${wiki} ${algo_outputdir}
	ENDTIME=${SECONDS}
	metric_name=metrics.algorunner.${wiki}.${snapshot}.seconds
        timestamp=$(date +%s)
	echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}
	

	# 2. Upload to HDFS
        echo "Publishing raw data to HDFS for ${wiki}"
	STARTTIME=${SECONDS}

	hdfs_imagerec=/user/${username}/imagerec
	spark_master_local='local[2]' # Use a local master to copy from the Driver to HDFS.
	spark2-submit --properties-file ${spark_config} --master ${spark_master_local} \
	  --files etl/schema.py \
	  etl/raw2parquet.py \
	  --wiki $wiki \
	  --snapshot ${monthly_snapshot} \
	  --source file://$(pwd)/${algo_outputdir}/${wiki}_${snapshot}_wd_image_candidates.tsv \
	  --destination ${hdfs_imagerec}/

	ENDTIME=${SECONDS}
	metric_name=metrics.hdfs.copyrawdata.${wiki}.${snapshot}.seconds
        timestamp=$(date +%s)
	echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}

	# 3. Update hive external table metadata
	echo "Updating Hive medatada for ${wiki}" 
	STARTTIME=${SECONDS}
	hive -hiveconf username=${username} -f ddl/external_imagerec.hql
done
	ENDTIME=${SECONDS}
	timestamp=$(date +%s)
	metric_name=metrics.hive.imagerec.${wiki}.${snapshot}.seconds
	echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}
# 4. Submit the Spark production data ETL
echo "Generating production data"


STARTTIME=${SECONDS}

hdfs_imagerec_prod=/user/${username}/imagerec_prod
spark2-submit --properties-file ${spark_config} --files etl/schema.py etl/transform.py \
	--snapshot ${monthly_snapshot} \
	--source ${hdfs_imagerec} \
	--destination ${hdfs_imagerec_prod} \
	--dataset-id ${run_id}
ENDTIME=${SECONDS}
metric_name=metrics.etl.transfrom.${snapshot}.second
timestamp=$(date +%s)
echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}


# 5. Update hive external table metadata (production)
STARTTIME=${SECONDS}
hive -hiveconf username=${username} -f ddl/external_imagerec_prod.hql
ENDTIME=${SECONDS}
metric_name=hive.imagerec_prod.${snapshot}
timestamp=$(date +%s)
echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}

# 6. Export production datasets
STARTIME=${SECONDS}
mkdir ${outputdir}
mkdir ${tsv_tmpdir}
for wiki in ${poc_wikis}; do
	# 1. First we collect data into a set of TSV files at location `output_path`.
        #    And we save the header (the command output) to a temp _header file.
        hive -hiveconf username=${username} -hiveconf output_path=${tsv_tmpdir}/${wiki}_${monthly_snapshot} -hiveconf wiki=${wiki} -hiveconf snapshot=${monthly_snapshot} -f ddl/export_prod_data.hql > ${tsv_tmpdir}/${wiki}_${monthly_snapshot}_header;
        # .2 Then we append all TSV files to _header, and collect them into a single-file dataset.
        cat ${tsv_tmpdir}/${wiki}_${monthly_snapshot}_header ${tsv_tmpdir}/${wiki}_${monthly_snapshot}/* > ${outputdir}/prod-${wiki}-${snapshot}-wd_image_candidates.tsv
done
rm -r ${tsv_tmpdir}
ENDTIME=${SECONDS}

echo "Datasets are available at $outputdir/"
metric_name=metrics.etl.export_prod_data.${snapshot}.seconds
timestamp=$(date +%s)
echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}

#7. Create search table parquet file
STARTTIME=${SECONDS}
hdfs_search_imagerec=/user/${username}/search_imagerec
spark2-submit --properties-file ${spark_config} --files etl/search_table.py \
	--snapshot ${monthly_snapshot} \
	--source ${hdfs_imagerec_prod} \
	--destination ${hdfs_search_imagerec}
metric_name=metrics.etl.search_table.${snapshot}.second
timestamp=$(date +%s)
echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}

#8 Update hive external table metadata (search_table)
STARTTIME=${SECONDS}
hive -hiveconf username=${username} -f ddl/external_search_imagerec.hql
ENDTIME=${SECONDS}
metric_name=hive.search_imagerec.${snapshot}
timestamp=$(date +%s)
echo "${timestamp},$(($ENDTIME - $STARTTIME))" >> ${metrics_dir}/${metric_name}

echo "Export summary"
cut -f 3,4 ${outputdir}/*.tsv | sort -k 1,2 | uniq -c 
